{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentive Conditional Neural Process (AttnCNP)\n",
    "\n",
    "```{figure} ../images/computational_graph_AttnCNPs.svg\n",
    "---\n",
    "width:  300em\n",
    "name: computational_graph_AttnCNPs\n",
    "alt: Computational graph of AttnCNP\n",
    "---\n",
    "Computational graph for Attentive Conditional Neural Processes.\n",
    "```\n",
    "\n",
    "In this notebook we will show how to train a AttnCNP on samples from GPs and images using our framework, as well as how to make nice visualizations.\n",
    "AttnCNPs are CNPFs that use use attention {cite}`bahdanau2014neural`for the aggregator (computational graph in {numref}`computational_graph_AttnCNPs`).\n",
    "We will follow quite closely the previous {doc}`CNP notebook <CNP>`, which thus contains a little more details than this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "\n",
    "N_THREADS = 8\n",
    "IS_FORCE_CPU = False  # Nota Bene : notebooks don't deallocate GPU memory\n",
    "\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Let's load all the data. For more details about the data and some samples, see the {doc}`data <Datasets>` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import get_all_gp_datasets, get_img_datasets\n",
    "\n",
    "# DATASETS\n",
    "# gp\n",
    "gp_datasets, gp_test_datasets, gp_valid_datasets = get_all_gp_datasets()\n",
    "# image\n",
    "img_datasets, img_test_datasets = get_img_datasets([\"celeba32\", \"mnist\", \"zsmms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the context target splitters, which given a data point will return the context set and target set by selecting randomly selecting some points and preprocessing them so that the features are in $[-1,1]$. \n",
    "We use the same as in {doc}`CNP notebook <CNP>`, namely all target points and uniformly sampling in $[0,50]$ and $[0,n\\_pixels * 0.3]$ for 1D and 2D respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from npf.utils.datasplit import (\n",
    "    CntxtTrgtGetter,\n",
    "    GetRandomIndcs,\n",
    "    GridCntxtTrgtGetter,\n",
    "    RandomMasker,\n",
    "    get_all_indcs,\n",
    "    no_masker,\n",
    ")\n",
    "from utils.data import cntxt_trgt_collate, get_test_upscale_factor\n",
    "\n",
    "# CONTEXT TARGET SPLIT\n",
    "get_cntxt_trgt_1d = cntxt_trgt_collate(\n",
    "    CntxtTrgtGetter(\n",
    "        contexts_getter=GetRandomIndcs(a=0.0, b=50), targets_getter=get_all_indcs,\n",
    "    )\n",
    ")\n",
    "get_cntxt_trgt_2d = cntxt_trgt_collate(\n",
    "    GridCntxtTrgtGetter(\n",
    "        context_masker=RandomMasker(a=0.0, b=0.3), target_masker=no_masker,\n",
    "    )\n",
    ")\n",
    "\n",
    "# for ZSMMS you need the pixels to not be in [-1,1] but [-1.75,1.75] (i.e 56 / 32) because you are extrapolating\n",
    "get_cntxt_trgt_2d_extrap = cntxt_trgt_collate(\n",
    "    GridCntxtTrgtGetter(\n",
    "        context_masker=RandomMasker(a=0, b=0.5),\n",
    "        target_masker=no_masker,\n",
    "        upscale_factor=get_test_upscale_factor(\"zsmms\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the models. For both the 1D and 2D case we will be using the following:\n",
    "* **Encoder** $\\mathrm{e}_{\\boldsymbol{\\theta}}$ : a 1-hidden layer MLP that encodes the features, followed by\n",
    "    * 1D : 2 hidden layer MLP that encodes each feature-value pair.\n",
    "    * 2D : two self attention layers each implemented as 8-headed attention, a skip connection, and two layer normalizations (as in {cite}`kim2019attentive`).[^selfattn]\n",
    "* **Aggregator** $\\mathrm{Agg}$: multi-head cross-attention layer.\n",
    "* **Decoder** $\\mathrm{d}_{\\boldsymbol{\\theta}}$: a 4 hidden layer MLP that predicts the distribution of the target value given the global representation and target context.\n",
    "\n",
    "All hidden representations will be of 128 dimensions.\n",
    "\n",
    "\n",
    "[^selfattn]: To be in line with {numref}`computational_graph_AttnCNPs`, the self attention layers should actually be in the aggregator instead of the encoder. Indeed, we apply the encoder to each context point separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from npf import AttnCNP\n",
    "from npf.architectures import MLP, merge_flat_input\n",
    "from utils.helpers import count_parameters\n",
    "\n",
    "R_DIM = 128\n",
    "KWARGS = dict(\n",
    "    r_dim=R_DIM,\n",
    "    attention=\"transformer\",  # multi headed attention with normalization and skip connections\n",
    "    XEncoder=partial(MLP, n_hidden_layers=1, hidden_size=R_DIM),\n",
    "    Decoder=merge_flat_input(  # MLP takes single input but we give x and R so merge them\n",
    "        partial(MLP, n_hidden_layers=4, hidden_size=R_DIM), is_sum_merge=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 1D case\n",
    "model_1d = partial(\n",
    "    AttnCNP,\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    XYEncoder=merge_flat_input(  # MLP takes single input but we give x and y so merge them\n",
    "        partial(MLP, n_hidden_layers=2, hidden_size=R_DIM), is_sum_merge=True,\n",
    "    ),\n",
    "    is_self_attn=False,\n",
    "    **KWARGS,\n",
    ")\n",
    "\n",
    "# image (2D) case\n",
    "model_2d = partial(\n",
    "    AttnCNP,\n",
    "    x_dim=2,\n",
    "    is_self_attn=True,  # no XYEncoder because using self attention\n",
    "    **KWARGS,\n",
    ")  # don't add y_dim yet because depends on data (colored or gray scale)\n",
    "\n",
    "n_params_1d = count_parameters(model_1d())\n",
    "n_params_2d = count_parameters(model_2d(y_dim=3))\n",
    "print(f\"Number Parameters (1D): {n_params_1d:,d}\")\n",
    "print(f\"Number Parameters (2D): {n_params_2d:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about all the possible parameters, refer to the docstrings of `AttnCNP` and the base class `NeuralProcessFamily`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Attentive conditional neural process. I.e. deterministic version of [1].\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x_dim : int\n",
      "        Dimension of features.\n",
      "\n",
      "    y_dim : int\n",
      "        Dimension of y values.\n",
      "\n",
      "    XYEncoder : nn.Module, optional\n",
      "        Encoder module which maps {x_transf_i, y_i} -> {r_i}. C.f. ConditionalNeuralProcess for more\n",
      "        details. Only used if `is_self_attn==False`.\n",
      "\n",
      "    attention : callable or str, optional\n",
      "        Type of attention to use. More details in `get_attender`.\n",
      "\n",
      "    attention_kwargs : dict, optional\n",
      "        Additional arguments to `get_attender`.\n",
      "\n",
      "    self_attention_kwargs : dict, optional\n",
      "        Additional arguments to `SelfAttention`.\n",
      "\n",
      "    is_self_attn : bool, optional\n",
      "        Whether to use self attention in the encoder. \n",
      "\n",
      "    kwargs :\n",
      "        Additional arguments to `NeuralProcessFamily`.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    [1] Kim, Hyunjik, et al. \"Attentive neural processes.\" arXiv preprint\n",
      "        arXiv:1901.05761 (2019).\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# AttnCNP Docstring\n",
    "\n",
    "print(AttnCNP.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The main function for training is `train_models` which trains a dictionary of models on a dictionary of datasets and returns all the trained models.\n",
    "See its docstring for possible parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Notes :\n",
    "- The following will either train all the models (`is_retrain=True`) or load the pretrained models (`is_retrain=False`)\n",
    "- it will use a (single) GPU if available\n",
    "- decrease the batch size if you don't have enough memory\n",
    "- 30 epochs should give you descent results for the GP datasets (instead of 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading RBF_Kernel/AttnCNP/run_0 ---\n",
      "\n",
      "RBF_Kernel/AttnCNP/run_0 | best epoch: None | train loss: -157.9417 | valid loss: None | test log likelihood: 149.158\n",
      "\n",
      "--- Loading Periodic_Kernel/AttnCNP/run_0 ---\n",
      "\n",
      "Periodic_Kernel/AttnCNP/run_0 | best epoch: None | train loss: 21.2395 | valid loss: None | test log likelihood: -25.4617\n",
      "\n",
      "--- Loading Noisy_Matern_Kernel/AttnCNP/run_0 ---\n",
      "\n",
      "Noisy_Matern_Kernel/AttnCNP/run_0 | best epoch: None | train loss: 87.5571 | valid loss: None | test log likelihood: -91.5147\n",
      "\n",
      "--- Loading Variable_Matern_Kernel/AttnCNP/run_0 ---\n",
      "\n",
      "Variable_Matern_Kernel/AttnCNP/run_0 | best epoch: None | train loss: -204.3232 | valid loss: None | test log likelihood: -4009.3233\n",
      "\n",
      "--- Loading All_Kernels/AttnCNP/run_0 ---\n",
      "\n",
      "All_Kernels/AttnCNP/run_0 | best epoch: None | train loss: 74.5616 | valid loss: None | test log likelihood: -116.8501\n",
      "\n",
      "--- Loading celeba32/AttnCNP/run_0 ---\n",
      "\n",
      "celeba32/AttnCNP/run_0 | best epoch: 43 | train loss: -4760.5214 | valid loss: -4968.2048 | test log likelihood: 4828.3025\n",
      "\n",
      "--- Loading mnist/AttnCNP/run_0 ---\n",
      "\n",
      "mnist/AttnCNP/run_0 | best epoch: 39 | train loss: -2311.3486 | valid loss: -2423.4288 | test log likelihood: 2262.2453\n",
      "\n",
      "--- Loading zsmms/AttnCNP/run_0 ---\n",
      "\n",
      "zsmms/AttnCNP/run_0 | best epoch: 1 | train loss: -415.7267 | valid loss: 52463.8337 | test log likelihood: -309088.0422\n"
     ]
    }
   ],
   "source": [
    "import skorch\n",
    "from npf import CNPFLoss\n",
    "from utils.ntbks_helpers import add_y_dim\n",
    "from utils.train import train_models\n",
    "\n",
    "KWARGS = dict(\n",
    "    is_retrain=False,  # whether to load precomputed model or retrain\n",
    "    criterion=CNPFLoss,\n",
    "    chckpnt_dirname=\"results/pretrained/\",\n",
    "    device=None,\n",
    "    lr=1e-3,\n",
    "    decay_lr=10,\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "\n",
    "# 1D\n",
    "trainers_1d = train_models(\n",
    "    gp_datasets,\n",
    "    {\"AttnCNP\": model_1d},\n",
    "    test_datasets=gp_test_datasets,\n",
    "    train_split=None,  # No need for validation as the training data is generated on the fly\n",
    "    iterator_train__collate_fn=get_cntxt_trgt_1d,\n",
    "    iterator_valid__collate_fn=get_cntxt_trgt_1d,\n",
    "    max_epochs=100,\n",
    "    **KWARGS\n",
    ")\n",
    "\n",
    "\n",
    "# 2D\n",
    "trainers_2d = train_models(\n",
    "    img_datasets,\n",
    "    add_y_dim({\"AttnCNP\": model_2d}, img_datasets),  # y_dim (channels) depend on data\n",
    "    test_datasets=img_test_datasets,\n",
    "    train_split=skorch.dataset.CVSplit(0.1),  # use 10% of training for valdiation\n",
    "    iterator_train__collate_fn=get_cntxt_trgt_2d,\n",
    "    iterator_valid__collate_fn=get_cntxt_trgt_2d,\n",
    "    datasets_kwargs=dict(\n",
    "        zsmms=dict(iterator_valid__collate_fn=get_cntxt_trgt_2d_extrap,)\n",
    "    ),  # for zsmm use extrapolation\n",
    "    max_epochs=50,\n",
    "    **KWARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Let's visualize how well the model performs in different settings.\n",
    "\n",
    "#### GPs Dataset\n",
    "\n",
    "Let's define a plotting function that we will use in this section. We'll reuse the same function defined in {doc}`CNP notebook <CNP>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import PRETTY_RENAMER, plot_multi_posterior_samples_1d\n",
    "from utils.visualize import giffify\n",
    "\n",
    "\n",
    "def multi_posterior_gp_gif(filename, trainers, datasets, seed=123, **kwargs):\n",
    "    giffify(\n",
    "        save_filename=f\"jupyter/gifs/{filename}.gif\",\n",
    "        gen_single_fig=plot_multi_posterior_samples_1d,  # core plotting\n",
    "        sweep_parameter=\"n_cntxt\",  # param over which to sweep\n",
    "        sweep_values=[1,2, 5, 7, 10, 15, 20, 30, 50, 100],\n",
    "        fps=1.5,  # gif speed\n",
    "        # PLOTTING KWARGS\n",
    "        trainers=trainers,\n",
    "        datasets=datasets,\n",
    "        is_plot_generator=True,  # plot underlying GP\n",
    "        is_plot_real=False,  # don't plot sampled / underlying function\n",
    "        is_plot_std=True,  # plot the predictive std\n",
    "        is_fill_generator_std=False,  # do not fill predictive of GP\n",
    "        pretty_renamer=PRETTY_RENAMER,  # pretiffy names of modulte + data\n",
    "        # Fix formatting for coherent GIF\n",
    "        plot_config_kwargs=dict(\n",
    "            set_kwargs=dict(ylim=[-3, 3]), rc={\"legend.loc\": \"upper right\"}, \n",
    "        ),\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Samples from a single GP\n",
    "\n",
    "First, let us visualize the AttnCNP when it is trained on samples from a single GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def filter_single_gp(d):\n",
    "    return {k: v for k, v in d.items() if (\"All\" not in k) and (\"Variable\" not in k)}\n",
    "\n",
    "\n",
    "multi_posterior_gp_gif(\n",
    "    \"AttnCNP_single_gp\",\n",
    "    trainers=filter_single_gp(trainers_1d),\n",
    "    datasets=filter_single_gp(gp_test_datasets),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../gifs/AttnCNP_single_gp.gif\n",
    "---\n",
    "width: 40em\n",
    "name: AttnCNP_single_gp\n",
    "alt: AttnCNP on single GPs\n",
    "---\n",
    "\n",
    "Posterior predictive of AttnCNPs (Blue line with shaded area for $\\mu \\pm \\sigma$) and the oracle GP (Green line with dashes for $\\mu \\pm \\sigma$) when conditioned on contexts points (Black) from an underlying function sampled from a GP. Each row corresponds to a different kernel and AttnCNP trained on samples for the corresponding GP. \n",
    "```\n",
    "\n",
    "Compared to CNPs ({numref}`CNP_single_gp`), {numref}`AttnCNP_single_gp` shows that the results are much better and that AttnCNP does not really suffer from underfitting.\n",
    "That being said, the results on the periodic kernel are still not great. Looking carefully at the Matern and RBF kernel, we also see that AttnCNP has a posterior predictive with \"kinks\", i.e., it is not very smooth. We believe that this happens because of the exponential in the attention. Namely the \"kinks\" appear when the AttnCNP the attention abruptly changes from one context point to the other. This hypothesis is supported by the fact that the kinks usually appear in the middle of 2 context points.\n",
    "\n",
    "Overall, AttnCNP performs quite well in this simple setting. Let us make the task slightly harder by conditioning on contexts points outside of the training regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_posterior_gp_gif(\n",
    "    \"AttnCNP_single_gp_extrap\",\n",
    "    trainers=filter_single_gp(trainers_1d),\n",
    "    datasets=filter_single_gp(gp_test_datasets),\n",
    "    left_extrap=-2,  # shift signal 2 to the right for extrapolation\n",
    "    right_extrap=2,  # shift signal 2 to the right for extrapolation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../gifs/AttnCNP_single_gp_extrap.gif\n",
    "---\n",
    "width: 40em\n",
    "name: AttnCNP_single_gp_extrap\n",
    "alt: AttnCNP on single GP\n",
    "---\n",
    "\n",
    "Same as {numref}`AttnCNP_single_gp` but with context points outside of the training regime (delimited by red dashes).\n",
    "```\n",
    "\n",
    "{numref}`AttnCNP_single_gp_extrap` shows that AttnCNP cannot perform well in this seemingly straightforward extension of {numref}`AttnCNP_single_gp`.\n",
    "The issue is two fold : (i) the network takes as input the *absolute* position of the features $\\mathbf{x}$ even though the desired kernel is stationary and thus only depends on the relative position of features; (ii) the absolute positions are in the extrapolation regime, which usually breaks in neural networks {cite}`dubois2019location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "###### ADDITIONAL 1D PLOTS ######\n",
    "\n",
    "### RBF ###\n",
    "def filter_rbf(d):\n",
    "    \"\"\"Select only data form RBF.\"\"\"\n",
    "    return {k: v for k, v in d.items() if (\"RBF\" in k)}\n",
    "\n",
    "\n",
    "multi_posterior_gp_gif(\n",
    "    \"AttnCNP_rbf_extrap\",\n",
    "    trainers=filter_rbf(trainers_1d),\n",
    "    datasets=filter_rbf(gp_test_datasets),\n",
    "    left_extrap=-2,  # shift signal 2 to the right for extrapolation\n",
    "    right_extrap=2,  # shift signal 2 to the right for extrapolation\n",
    ")\n",
    "\n",
    "### Varying hyperparam ###\n",
    "def filter_hyp_gp(d):\n",
    "    return {k: v for k, v in d.items() if (\"Variable\" in k)}\n",
    "\n",
    "\n",
    "multi_posterior_gp_gif(\n",
    "    \"AttnCNP_vary_gp\",\n",
    "    trainers=filter_hyp_gp(trainers_1d),\n",
    "    datasets=filter_hyp_gp(gp_test_datasets),\n",
    "    model_labels=dict(main=\"Model\", generator=\"Fitted GP\"),\n",
    ")\n",
    "\n",
    "### All kernels ###\n",
    "# data with varying kernels simply merged single kernels\n",
    "single_gp_datasets = filter_single_gp(gp_test_datasets)\n",
    "\n",
    "# use same trainer for all, but have to change their name to be the same as datasets\n",
    "base_trainer_name = \"All_Kernels/AttnCNP/run_0\"\n",
    "trainer = trainers_1d[base_trainer_name]\n",
    "replicated_trainers = {}\n",
    "for name in single_gp_datasets.keys():\n",
    "    replicated_trainers[base_trainer_name.replace(\"All_Kernels\", name)] = trainer\n",
    "\n",
    "multi_posterior_gp_gif(\n",
    "    \"AttnCNP_kernel_gp\",\n",
    "    trainers=replicated_trainers,\n",
    "    datasets=single_gp_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Dataset\n",
    "\n",
    "Let us now look at images. We again will use the same plotting function defined in {doc}`CNP notebook <CNP>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import plot_multi_posterior_samples_imgs\n",
    "from utils.visualize import giffify\n",
    "\n",
    "def multi_posterior_imgs_gif(filename, trainers, datasets, seed=123, **kwargs):\n",
    "    giffify(\n",
    "        save_filename=f\"jupyter/gifs/{filename}.gif\",\n",
    "        gen_single_fig=plot_multi_posterior_samples_imgs,  # core plotting\n",
    "        sweep_parameter=\"n_cntxt\",  # param over which to sweep\n",
    "        sweep_values=[\n",
    "            0,  \n",
    "            0.005,\n",
    "            0.01,\n",
    "            0.02,\n",
    "            0.05,\n",
    "            0.1,\n",
    "            0.15,\n",
    "            0.2,\n",
    "            0.3,\n",
    "            0.5,\n",
    "            \"hhalf\",  # horizontal half of the image\n",
    "            \"vhalf\",  # vertival half of the image\n",
    "        ],\n",
    "        fps=1.5,  # gif speed\n",
    "        # PLOTTING KWARGS\n",
    "        trainers=trainers,\n",
    "        datasets=datasets,\n",
    "        n_plots=3,  # images per datasets\n",
    "        is_plot_std=True,  # plot the predictive std\n",
    "        pretty_renamer=PRETTY_RENAMER,  # pretiffy names of modulte + data\n",
    "        plot_config_kwargs={\"font_scale\":0.7},\n",
    "        # Fix formatting for coherent GIF\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the CNP when it is trained on samples from different image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_posterior_imgs_gif(\n",
    "    \"AttnCNP_img\", trainers=trainers_2d, datasets=img_test_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../gifs/AttnCNP_img.gif\n",
    "---\n",
    "width: 40em\n",
    "name: AttnCNP_img\n",
    "alt: CNP on CelebA and MNIST\n",
    "---\n",
    "\n",
    "Mean and std of the posterior predictive of an AttnCNP for CelebA $32\\times32$, MNIST, and ZSMM for different context sets.\n",
    "```\n",
    "\n",
    "Similarly to the case of GPs, {numref}`AttnCNP_img` shows that the AttnCNP performs quite well in when extrapolation is not needed (Celeba32 and MNIST) but fails otherwise (ZSMM).\n",
    "\n",
    "Here are more samples, corresponding to specific percentiles of the test log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from utils.visualize import plot_qualitative_with_kde\n",
    "\n",
    "n_trainers = len(trainers_2d)\n",
    "for i, (k, trainer) in enumerate(trainers_2d.items()):\n",
    "    data_name = k.split(\"/\")[0]\n",
    "    model_name = k.split(\"/\")[1]\n",
    "    dataset = img_test_datasets[data_name]\n",
    "\n",
    "    plot_qualitative_with_kde(\n",
    "        [PRETTY_RENAMER[model_name], trainer],\n",
    "        dataset,\n",
    "        figsize=(7, 5),\n",
    "        percentiles=[1, 10, 20, 30, 50, 100],  # desired test percentile\n",
    "        height_ratios=[1, 5],  # kde / image ratio\n",
    "        is_smallest_xrange=True,  # rescale X axis based on percentile\n",
    "        h_pad=-1,  # padding\n",
    "        title=PRETTY_RENAMER[data_name],\n",
    "        upscale_factor=get_test_upscale_factor(data_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "###### ADDITIONAL 2D PLOTS ######\n",
    "\n",
    "### Interpolation ###\n",
    "def filter_interpolation(d):\n",
    "    \"\"\"Filter out zsmms which requires extrapolation.\"\"\"\n",
    "    return {k: v for k, v in d.items() if \"zsmms\" not in k}\n",
    "\n",
    "\n",
    "multi_posterior_imgs_gif(\n",
    "    \"AttnCNP_img_interp\",\n",
    "    trainers=filter_interpolation(trainers_2d),\n",
    "    datasets=filter_interpolation(img_test_datasets),\n",
    ")\n",
    "\n",
    "### Extrapolation  ###\n",
    "def filter_interpolation(d):\n",
    "    \"\"\"Filter out zsmms which requires extrapolation.\"\"\"\n",
    "    return {k: v for k, v in d.items() if \"zsmms\" in k}\n",
    "\n",
    "\n",
    "multi_posterior_imgs_gif(\n",
    "    \"AttnCNP_img_extrap\",\n",
    "    trainers=filter_interpolation(trainers_2d),\n",
    "    datasets=filter_interpolation(img_test_datasets),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

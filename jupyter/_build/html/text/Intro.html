

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Neural Process Family &#8212; Neural Process Family</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/proof.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Conditional NPFs" href="CNPFs.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.gif" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neural Process Family</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="active">
    <a href="#">Neural Process Family</a>
  </li>
  <li class="">
    <a href="CNPFs.html">Conditional NPFs</a>
  </li>
  <li class="">
    <a href="LNPFs.html">Latent NPFs</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Advanced</p>
</li>
  <li class="">
    <a href="Theory.html">Theory</a>
  </li>
  <li class="">
    <a href="Training.html">Training</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Reproducibility</p>
</li>
  <li class="">
    <a href="../reproducibility/Datasets.html">Datasets</a>
  </li>
  <li class="">
    <a href="../reproducibility/CNP.html">CNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/AttnCNP.html">AttnCNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/ConvCNP.html">ConvCNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/LNP.html">LNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/AttnLNP.html">AttnLNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/ConvLNP.html">ConvLNP</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Reference</p>
</li>
  <li class="">
    <a href="Related.html">Related</a>
  </li>
  <li class="">
    <a href="../zbibliography.html">Bibliography</a>
  </li>
  <li class="">
    <a href="https://github.com/YannDubs/Neural-Process-Family">GitHub Repo<i class="fas fa-external-link-alt"></i></a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/text/Intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/YannDubs/Neural-Process-Family"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#meta-learning-under-uncertainty" class="nav-link">Meta Learning Under Uncertainty</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#modeling-stochastic-processes" class="nav-link">Modeling Stochastic Processes</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#npf-members" class="nav-link">NPF Members</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#training" class="nav-link">Training</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#usecases" class="nav-link">Usecases</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#properties" class="nav-link">Properties</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-process-family">
<h1>Neural Process Family<a class="headerlink" href="#neural-process-family" title="Permalink to this headline">Â¶</a></h1>
<p>In this tutorial, we introduce the <strong>Neural Process Family</strong> (NPF). Neural Processes are a family of deep learning architectures that model <strong>predictive stochastic processes</strong> with neural networks. The framework is quite general, and NPF members admit flexible, neural network based parameterisations, opening them up to many important settings. For example, in addition to allowing parameterisations for standard structured data such as discrete time-series and images, they also admit parameterisations for continuous domains, making them useful in settings where deep learning is typically not considered, such as continuous-time regression and spatio-temporal modelling.</p>
<div class="figure align-default" id="convlnp-norbf-gp-extrap">
<a class="reference internal image-reference" href="../_images/ConvLNP_norbf_gp_extrap.gif"><img alt="Samples from ConvLNP trained on GPs" src="../_images/ConvLNP_norbf_gp_extrap.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Sampled functions from ConvLNPs (Blue) and the oracle GP (Green) with Periodic or Noisy Matern kernel.</span><a class="headerlink" href="#convlnp-norbf-gp-extrap" title="Permalink to this image">Â¶</a></p>
</div>
<div class="figure align-default" id="convcnp-superes-intro">
<a class="reference internal image-reference" href="../_images/ConvCNP_superes.png"><img alt="Increasing image resolution with ConvCNP" src="../_images/ConvCNP_superes.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Increasing the resolution of <span class="math notranslate nohighlight">\(16 \times 16\)</span> CelebA to <span class="math notranslate nohighlight">\(128 \times 128\)</span> with a ConvCNP.</span><a class="headerlink" href="#convcnp-superes-intro" title="Permalink to this image">Â¶</a></p>
</div>
<p>The goal of this tutorial is to provide a gentle introduction to the NPF. Our approach is to walk through several prominent members of the NPF, highlighting their key advantages and drawbacks. By accompanying the exposition with code and examples, we hope to (i) make the design choices and tradeoffs associated with the different models clear and intuitive, and (ii) demonstrate both the simplicity of the framework and its broad applicability.</p>
<p>Throughout the tutorial, we consider two parallel narratives:</p>
<ol class="simple">
<li><p><strong>Probabilistic meta-learning</strong>: NPFs are naturally geared for the meta-learning setting, which has recently been demonstrated to be extremely useful in many important settings. Arguably, the most prominent applications of meta-learning arise when data for each task is sparse, motivating the need to account for <strong>uncertainty</strong>. As we shall see, NPF models provide an elegant framework for modelling uncertainty, both in our predictive distributions and inferences associated with particular tasks.</p></li>
<li><p><strong>Modelling stochastic processes</strong>: A more formal treatment can be achieved by considering maps from a space of finite datasets to a space of predictive stochastic processes. A general view of the NPF is as <em>deploying and training neural networks to approximate maps of this form</em>. While the majority of this tutorial avoids such mathematical constructions, this framework is useful for formalising (and proving) important statements about the NPF. As such, we allude to this view when appropriate, and provide more complete derivations and proofs in INSERT REF for the interested reader.</p></li>
</ol>
<div class="figure align-default" id="npfs">
<a class="reference internal image-reference" href="../_images/NPFs.gif"><img alt="Schematic representation of CNPF computations" src="../_images/NPFs.gif" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Schematic representation of NPF computations from <a class="reference external" href="https://www.martagarnelo.com/conditional-neural-processes">Marta Garnelo</a>.</span><a class="headerlink" href="#npfs" title="Permalink to this image">Â¶</a></p>
</div>
<p><a class="reference internal" href="#npfs"><span class="std std-numref">Fig. 3</span></a> [â¦ high level intuition ? â¦]</p>
<div class="section" id="meta-learning-under-uncertainty">
<h2>Meta Learning Under Uncertainty<a class="headerlink" href="#meta-learning-under-uncertainty" title="Permalink to this headline">Â¶</a></h2>
<p>[â¦keep?â¦]</p>
</div>
<div class="section" id="modeling-stochastic-processes">
<h2>Modeling Stochastic Processes<a class="headerlink" href="#modeling-stochastic-processes" title="Permalink to this headline">Â¶</a></h2>
<p>If you think of neural networks as a way of approximating a function <span class="math notranslate nohighlight">\(f : \mathcal{X} \to \mathcal{Y}\)</span>, then you can think of NPFs as a way of modeling a <em>distribution</em> over functions <span class="math notranslate nohighlight">\(f \sim P_{\mathcal{F}}\)</span> (a stochastic process) conditioned on a certain set of points.</p>
<p>Specifically, we want to model a distribution over <strong>target</strong> values <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}} := \{y^{(t)}\}_{t=1}^T\)</span> conditioned on a set of corresponding target features <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}} := \{x^{(t)}\}_{t=1}^T\)</span> and a <strong>context</strong> set of feature-value pairs <span class="math notranslate nohighlight">\(\mathcal{C} := \{(x^{(c)}, y^{(c)})\}_{c=1}^C\)</span>.
We call such distribution <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span> the <strong>posterior predictive</strong> as it predicts <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}}\)</span> after (a <em>posteriori</em>) conditioning on <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you sampled <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}}\)</span> according to <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span>, for all possible features <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}}=\mathcal{X}\)</span> then you effectively sampled an entire function.</p>
</div>
<p>Outside of NPFs, stochastic processes are typically modelled by choosing the form of the distribution <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span> and then using rules of probabilities to infer different terms such as the posterior predictive <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C}) = \frac{p(\mathbf{y}_{\mathcal{T}},\mathbf{y}_{\mathcal{C}}|\mathbf{x}_{\mathcal{T}}, \mathbf{x}_\mathcal{C})}{p(\mathbf{y}_\mathcal{C}|\mathbf{x}_\mathcal{C})}\)</span>.
For example, this is the approach of <a class="reference external" href="https://distill.pub/2019/visual-exploration-gaussian-processes/">Gaussian Processes</a> (GPs), which uses a multivariate normal distribution for any <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span>.
To define a proper stochastic process using <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span>, it essentially needs to satisfy two consistency conditions (<a class="reference external" href="https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem">Kolmogorov existence theorem</a>) that can informally be summarized as follows :</p>
<ul class="simple">
<li><p><strong>Permutation invariance</strong>
<span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})=p(\pi(\mathbf{y})|\pi(\mathbf{x}))\)</span> for all permutations <span class="math notranslate nohighlight">\(\pi\)</span> on <span class="math notranslate nohighlight">\(1, ..., |\mathbf{y}|\)</span>. In other words, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are sets and should thus be unordered.</p></li>
<li><p><strong>Consistent under marginalizaion</strong>
<span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})= \int p(\mathbf{y}'|\mathbf{x}') p(\mathbf{y}|\mathbf{x},\mathbf{y}',\mathbf{x}') d\mathbf{y}'\)</span>, for all <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span>.</p></li>
</ul>
<p>By ensuring that these two condition hold, one can use standard probability rules and inherits nice mathematical properties.
Unfortunately stochastic processes are usually computationally inefficient.
For example predicting values of a target set using GPs takes time which is cubic in the context set <span class="math notranslate nohighlight">\(\mathcal{O}(|\mathcal{C}|^3)\)</span>.</p>
<div class="figure align-default" id="computational-graph-npfs">
<a class="reference internal image-reference" href="../_images/computational_graph_NPFs.svg"><img alt="high level computational graph of NPF" src="../_images/computational_graph_NPFs.svg" width="300em" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">High level computational graph of the Neural Process Family.</span><a class="headerlink" href="#computational-graph-npfs" title="Permalink to this image">Â¶</a></p>
</div>
<p>In contrast, the core idea of NPFs is to directly model the posterior predictive using neural networks <span class="math notranslate nohighlight">\(p( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathcal{C}) \approx q_{\boldsymbol \theta}(\mathbf{y}_{\mathcal{T}}  | \mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span>.
It does so by encoding all the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> to a global representation <span class="math notranslate nohighlight">\(R\)</span> and then decoding from it for each target point.
The core of NPFs is that the encoder is permutation invariant, i.e. treats <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> as a set, to satisfy the first consistency condition of stochastic processes.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Although all NPFs use permutation invariant encoders to satisfy the first consistency condition, they usually are not consistent under marginalization and thus arenât proper stochastic processes.
As a result, they sacrifice many nice mathematical properties.
[â¦ Practical issues ? â¦]</p>
</div>
</div>
<div class="section" id="npf-members">
<h2>NPF Members<a class="headerlink" href="#npf-members" title="Permalink to this headline">Â¶</a></h2>
<p>The major difference across the NPF, lies in the chosen encoder.
Usually (see <a class="reference internal" href="#npfs"><span class="std std-numref">Fig. 3</span></a>) it first encode each context points separately (âlocal encodingâ) <span class="math notranslate nohighlight">\(x^{(c)}, y^{(c)} \mapsto R^{(c)}\)</span> and then aggregates those in a representation for the entire context set <span class="math notranslate nohighlight">\(\mathrm{Agg}\left(\{R^{(c)}\}_{c=1}^{C} \right) \mapsto R\)</span> .
Conditioned on this representation, all the target values become independent (factorization assumption) <span class="math notranslate nohighlight">\(q_{\boldsymbol\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, R) =  \prod_{t=1}^{T} q_{\boldsymbol\theta}(y^{(t)} |  x^{(t)}, R)\)</span>.</p>
<p>NPFs can essentially be categorized into 2 sub-families depending on whether the global representation is treated as a latent variable (<a class="reference internal" href="LNPFs.html"><span class="doc">Latent NPFs</span></a><a class="footnote-reference brackets" href="#lnps" id="id1">1</a>) or not (<a class="reference internal" href="CNPFs.html"><span class="doc">Conditional NPFs</span></a>).
Inside each subfamily, the member essentially differ in how the representation is generated.
As we have already seen, the encoder is always permutation invariant. Specifically the aggregator <span class="math notranslate nohighlight">\(\mathrm{Agg}\)</span> can be arbitrarily complex but is invariant to all permutations <span class="math notranslate nohighlight">\(\pi\)</span> on <span class="math notranslate nohighlight">\(1, ..., C\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-permut-inv">
<span class="eqno">(1)<a class="headerlink" href="#equation-permut-inv" title="Permalink to this equation">Â¶</a></span>\[\begin{align}
\mathrm{Agg}\left( \{R^{(c)}\}_{c=1}^{C} \right)=\mathrm{Agg}\left(\pi\left(\{R^{(c)}\}_{c=1}^{C} \right)\right)
\end{align}\]</div>
<div class="dropdown caution admonition">
<p class="admonition-title">Theory</p>
<p>[Talk about deep sets]</p>
</div>
<p>Hereâs an overly simplified summary of the different NPFs that we will consider in this directory:</p>
<p>[â¦ keep? â¦]</p>
<table class="table" id="summary-npf">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Summary of different NPFs</span><a class="headerlink" href="#summary-npf" title="Permalink to this table">Â¶</a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"><p>Model</p></th>
<th class="head"><p>Aggregator</p></th>
<th class="head"><p>Stationarity</p></th>
<th class="head"><p>Expressivity</p></th>
<th class="head"><p>Extrapolation</p></th>
<th class="head"><p>Comp. Complexity</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="../reproducibility/CNP.html"><span class="doc">CNP</span></a> <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018conditional" id="id2">[GRM+18]</a>,<a class="reference internal" href="../reproducibility/LNP.html"><span class="doc">LNP</span></a> <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id3">[GSR+18]</a></p></th>
<td><p>mean</p></td>
<td></td>
<td></td>
<td></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(T+C)\)</span></p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference internal" href="../reproducibility/AttnCNP.html"><span class="doc">AttnCNP</span></a> <a class="footnote-reference brackets" href="#attncnp" id="id4">2</a>, <a class="reference internal" href="../reproducibility/AttnLNP.html"><span class="doc">AttnLNP</span></a> <a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id5">[KMS+19]</a></p></th>
<td><p>attention</p></td>
<td></td>
<td><p>True</p></td>
<td></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(C(T+C))\)</span></p></td>
</tr>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="../reproducibility/ConvCNP.html"><span class="doc">ConvCNP</span></a> <a class="bibtex reference internal" href="../zbibliography.html#gordon2019convolutional" id="id6">[GBF+19]</a>,<a class="reference internal" href="../reproducibility/ConvLNP.html"><span class="doc">ConvLNP</span></a>  <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id7">[FBG+20]</a>.</p></th>
<td><p>convolution</p></td>
<td><p>True</p></td>
<td><p>True</p></td>
<td><p>True</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(U(T+C))\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">Â¶</a></h2>
<p>[â¦MLE + meta dataâ¦]</p>
<p>The parameters are estimated by minimizing the expectation of the negative conditional conditional log likelihood (or approximation thereof for LNPFs):</p>
<div class="math notranslate nohighlight" id="equation-training">
<span class="eqno">(2)<a class="headerlink" href="#equation-training" title="Permalink to this equation">Â¶</a></span>\[\mathrm{NL}\mathcal{L}(\boldsymbol{\theta}) := -\mathbb{E}_{\mathrm{X}_\mathcal{T}} \left[ \mathbb{E}_{\mathrm{Y}_\mathcal{T}} \left[ \mathbb{E}_{\mathcal{C}} \left[ \log q_{\boldsymbol\theta} \left(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C} \right)\right] \right]\right]\]</div>
<p>We optimize it using stochastic gradient descent:</p>
<ol class="simple">
<li><p>Sample size of context set <span class="math notranslate nohighlight">\(C \sim \mathrm{Unif}(0,|\mathcal{X}|)\)</span></p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(C\)</span> context features <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{C}} \sim p(\mathbf{x}_{\mathcal{C}})\)</span>.</p></li>
<li><p>Sample associated values <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{C}} \sim p(\mathbf{y}_{\mathcal{C}} | \mathbf{x}_{\mathcal{C}})\)</span>.</p></li>
<li><p>Do the the same for the target set <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}},\mathbf{y}_{\mathcal{T}}\)</span> <a class="footnote-reference brackets" href="#sampletargets" id="id8">3</a>.</p></li>
<li><p>Compute the MC gradients of the negative log likelihood <span class="math notranslate nohighlight">\(- \nabla_{\pmb \theta} \sum_{t=1}^{T} \log p_{\boldsymbol \theta}(y^{(t)} | \mathbf{y}_\mathcal{C}; \mathbf{x}_\mathcal{C}, x^{(t)})\)</span></p></li>
<li><p>Backpropagate</p></li>
</ol>
<p>In practice, it means that training NPFs requires a dataset over sets of points.
This contrasts with usual neural network training which requires only a dataset of points.
In deep learning terms, we would say that it is trained through a <em>meta-learning</em> procedure.</p>
</div>
<div class="section" id="usecases">
<h2>Usecases<a class="headerlink" href="#usecases" title="Permalink to this headline">Â¶</a></h2>
<p>[â¦to doâ¦]</p>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">Â¶</a></h2>
<p>[â¦keep?â¦]</p>
<p>NPFs generally have the following desirable properties :</p>
<ul class="simple">
<li><p>â <strong>Preserve permutation invariance</strong> as with stochastic processes. This comes from the permutation invariance of <span class="math notranslate nohighlight">\(\mathrm{Agg}\)</span> (Eq. <a class="reference internal" href="#equation-permut-inv">(1)</a>) and the factorization assumption (Eq. <a class="reference internal" href="CNPFs_sketch.html#equation-formal">()</a>).
for all permutations <span class="math notranslate nohighlight">\(\pi_{\mathcal{T}}: |\mathbf{y}_{\mathcal{T}}| \to |\mathbf{y}_{\mathcal{T}}|\)</span> and <span class="math notranslate nohighlight">\(\pi_{\mathcal{C}}: |\mathbf{y}| \to |\mathbf{y}|\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_{\boldsymbol \theta}(\mathbf{y}_{\mathcal{T}}  | \mathbf{x}_{\mathcal{T}}, \mathcal{C}) = p_{\boldsymbol \theta}(\pi_{\mathcal{T}}(\mathbf{y})|\pi_{\mathcal{T}}(\mathbf{x}), \pi_{\mathcal{C}}(\mathcal{C}))
\]</div>
<ul class="simple">
<li><p>â <strong>Data Driven Expressivity</strong>. NPs require specification of prior knowledge through neural network architectures rather than a kernel function (like in GPs).
The former is usually less restrictive due to its large amount of parameters and removing the need of satisfying certain mathematical properties.
Intuitively, the NPs learn an âimplicit kernel functionâ from the data.</p></li>
<li><p>â <strong>Test-Time Scalability</strong>. Although the computational complexity depends on the NPF they are usually more computationally efficient (at test time) than proper stochastic processes.
Typically they will be linear or quadratic in the context set instead of cubic as with GPs.
[intuituin for gain????]</p></li>
</ul>
<p>These advantages come at the cost of the following disadvantages</p>
<ul class="simple">
<li><p>â <strong>Lack of consistency under marginalizaion</strong>. So NPFs are not proper stochastic processes.
This essentially means that even if you had infinite computational power (to be able to marginalize) and sampled points autoregressively, the order in which you do it would change the distribution over targets.
Formally, there exists <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> such that:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C}) \neq \int p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C},\mathbf{y}',\mathbf{x}')  
p(\mathbf{y}'|\mathbf{x}', \mathcal{C}) d\mathbf{y}'
\]</div>
<ul class="simple">
<li><p>â <strong>The need for large data</strong>. Learning requires collecting and training on a large dataset of target and context points sampled from different functions. I.e. a large dataset of datasets.</p></li>
<li><p>â <span class="math notranslate nohighlight">\(\sim\)</span> <strong>Lack of smoothness</strong>. Due to highly non linear behaviour of neural networks and the factorized form of the predictive distribution (Eq. <a class="reference internal" href="CNPFs_sketch.html#equation-formal">()</a>), the output tends to be non-smooth compared to a GP.
This is less true in newer NPFs such as ConvCNP <a class="bibtex reference internal" href="../zbibliography.html#gordon2019convolutional" id="id9">[GBF+19]</a>.</p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="lnps"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>In the literature the latent neural processes are just called neural processes. I use âlatentâ to distinguish them with the neural process family as a whole.</p>
</dd>
<dt class="label" id="attncnp"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p><a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id10">[KMS+19]</a> only introduced the latent variable model, but one can easily drop the latent variable if not needed.</p>
</dd>
<dt class="label" id="sampletargets"><span class="brackets"><a class="fn-backref" href="#id8">3</a></span></dt>
<dd><p>Instead of sampling targets, we will often use all the available targets <span class="math notranslate nohighlight">\(\mathbf{x}_\mathcal{T} = \mathcal{X}\)</span>. For example, in images the targets will usually be all pixels.</p>
</dd>
</dl>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='right-next' id="next-link" href="CNPFs.html" title="next page">Conditional NPFs</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yann Dubois and Jonathan Gordon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>